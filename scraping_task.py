# -*- coding: utf-8 -*-
"""Scraping task

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qHEyxKCnZKjHYpWrc-HJuoFTbiDK7bEQ

# COI Disclosure Statement Scraping Task
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# # Ubuntu no longer distributes chromium-browser outside of snap
# #
# # Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap
# 
# # Add debian buster
# cat > /etc/apt/sources.list.d/debian.list <<'EOF'
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main
# EOF
# 
# # Add keys
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A
# 
# apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg
# apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg
# apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg
# 
# # Prefer debian repo for chromium* packages only
# # Note the double-blank lines between entries
# cat > /etc/apt/preferences.d/chromium.pref << 'EOF'
# Package: *
# Pin: release a=eoan
# Pin-Priority: 500
# 
# 
# Package: *
# Pin: origin "deb.debian.org"
# Pin-Priority: 300
# 
# 
# Package: chromium*
# Pin: origin "deb.debian.org"
# Pin-Priority: 700
# EOF
# 
# # Install chromium and chromium-driver
# apt-get update
# apt-get install chromium chromium-driver
# 
# # Install selenium
# pip install selenium beautifulsoup4
# pip install openai
# pip install undetected-chromedriver
# 
#

# We are going to use Selenium and BeautifulSoup (BS4) a lot in extracting COI from the website.
# You don't need to fully understand how things work for now, because we set it up for you in advance.
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
import time
from bs4 import BeautifulSoup
import pandas as pd
from urllib.request import urlopen, Request
import requests
import undetected_chromedriver as uc

chrome_options = Options()
user_agent = 'Chrome/113.0.5672.24'
chrome_options.add_argument(f'user-agent={user_agent}')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument("--incognito")
chrome_options.add_argument('--window-size=1920,1080')
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument("--enable-javascript")
chrome_options.add_argument('--allow-running-insecure-content')
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)
chrome_options.add_argument('--disable-blink-features=AutomationControlled')
DRIVER_PATH = 'chromedriver'
driver = webdriver.Chrome(options=chrome_options, executable_path=DRIVER_PATH)

# driver = uc.Chrome()
# driver.get('https://distilnetworks.com')

"""#British Medical Journal

Can get it to work on all articles, need to iterate through journal now
"""

#driver.get('https://www.bmj.com/content/380/bmj-2022-073711')
#driver.get('https://www.bmj.com/content/359/bmj.j5058')
# driver.get('https://www.bmj.com/content/343/bmj.d6792')
driver.get('https://www.bmj.com/search/advanced/toc_section%3AResearch%20numresults%3A10%20sort%3Apublication-date%20direction%3Adescending%20format_result%3Astandard')
x = driver.page_source
soup = BeautifulSoup(x, 'html.parser')

articleList = soup.find('ul', {'class':'highwire-search-results-list'})

def getArticle(link):
    articleList = soup.find('ul', {'class':'highwire-search-results-list'})

funding = soup.find('li', {'class': 'fn-conflict'}).get_text()
disclosure = soup.find('li', {'class': 'fn-financial-disclosure'}).get_text()
coi = f'{disclosure} {funding}'
print(coi)

"""#Journal of the Royal Society of Medicine (through SagePub)"""

# driver.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS);
# driver.get('https://journals.sagepub.com/doi/full/10.1177/01410768211073923')
# driver.get('https://journals.sagepub.com/doi/full/10.1177/01410768221131897')
# driver.get('https://journals.sagepub.com/toc/jrsb/115/12')
# driver.implicitly_wait(45)
# driver.get('https://journals.sagepub.com/loi/jrsb/group/d2020.y2022')
# x = driver.page_source
# soup = BeautifulSoup(x, 'html.parser')

def getDriver(link):
    # driver.implicitly_wait(45)
    driver.get(link)
    x = driver.page_source
    soup = BeautifulSoup(x, 'html.parser')
    return soup

link = 'https://journals.sagepub.com/loi/jrsb/group/d2020.y2022'
soup = getDriver(link)
print(soup)
issueList = soup.find('div', {'class':'loi_issues'}).findChildren()
for child in issueList:
    reference = child.find('a', {'data-id':'loi-issue-link'}).get('href')
    url = 'https://journals.sagepub.com' + reference
    getArticle(url)

def COI():
    for x in range(3, 10):
        url = 'https://journals.sagepub.com/loi/jrsb/group/d2010.y201' + str(x)
        getIssue(url)
    for x in range(0, 4):
        url = 'https://journals.sagepub.com/loi/jrsb/group/d2020.y202' + str(x)
        getIssue(url)

def getIssue(link):
    soup = getDriver(link)
    issueList = soup.find('div', {'class':'loi_issues'}).findChildren()
    for child in issueList:
        reference = child.find('a', {'data-id':'loi-issue-link'}).get('href')
        url = 'https://journals.sagepub.com' + reference
        getArticle(url)
    return

def getDisclosure(link):
    soup = getDriver(link)
    disclosureText = soup.find('h2',string='Competing Interests').find_next_sibling().get_text()
    fundingText = soup.find('h2', string='Funding').find_next_sibling().get_text()
    coi = f'Disclosure: {disclosureText} Funding: {fundingText}'
    print(coi)
    return coi

def getArticle(link):
    soup = getDriver(link)
    articles = soup.find_all('span', string='Research article')
    for child in articles:
        reference = child.find_parent().find_parent().find('a', {'data-id':'toc-article-title'}).get('href')
        url = 'https://journals.sagepub.com' + reference
        getDisclosure(url)
        print(link)

COI()

"""#Canadian Medical Association Journal"""

# driver = uc.Chrome()
driver.get('https://www.cmaj.ca/content/by/section/research')
x = driver.page_source
soup = BeautifulSoup(x, 'html.parser')

def getDisclosure(link):
    time.sleep(15)
    driver.get(link)
    x = driver.page_source
    soup = BeautifulSoup(x, 'html.parser')
    DOI = soup.find('span', {'class':'highwire-cite-metadata-doi highwire-cite-metadata'}).get_text()
    funding = soup.find_all('strong', string='Funding:')[0].find_parent().get_text()
    disclosure = soup.find_all('strong', string='Competing interests:')[0].find_parent().get_text()
    if disclosure == 'Competing interests: None declared.':
        print('ignored')
        return
    else:
        coi = f'{DOI} {funding} {disclosure}'
        print(coi)
        return f'{DOI} {disclosure}'

listCOI = []
def getArticles(link):
    time.sleep(15)
    driver.get(link)
    x = driver.page_source
    soup = BeautifulSoup(x, 'html.parser')
    firstArticle = soup.find('li', {'class': 'first odd'})
    articleList = firstArticle.findParent().findChildren('li')
    for child in articleList:
        year = child.find('span', {'class':'highwire-cite-metadata-date highwire-cite-metadata'}).get_text()
        if '2020' in year:
            return
        else:
            link = child.find('a', {'class':'highwire-cite-linked-title'}).get('href')
            link = 'http://cmaj.ca' + link
            listCOI.append(getDisclosure(link))
    url = 'http://cmaj.ca' + soup.find('a', {'class':'link-icon link-icon-after'}).get('href')
    getArticles(url)

getArticles('https://www.cmaj.ca/content/by/section/research')

"""#QJM: An International Journal of Medicine"""

driver.get('https://academic.oup.com/qjmed/search-results?f_ContentSubTypeDisplayName=Research+Article&fl_SiteID=5416&rg_ArticleDate=01/01/2020%20TO%2012/31/2023&dateFilterType=range&noDateTypes=true&rg_SearchResultsPublicationDate=01/01/2020%20TO%2012/31/2023&rg_VersionDate=01/01/2020%20TO%2012/31/2023')
# driver.get('https://academic.oup.com/qjmed/article/112/7/497/5369095#137415904')
# driver.get('https://academic.oup.com/qjmed/article/112/1/29/5126239')
# driver.get('https://academic.oup.com/qjmed/article/114/6/381/5863257#304793952')
# driver.get('https://academic.oup.com/qjmed/article/116/3/205/6759384#399910601')
x = driver.page_source
soup = BeautifulSoup(x, 'html.parser')

def getDisclosure(link):
    driver.get(link)
    x = driver.page_source
    article = BeautifulSoup(x, 'html.parser')
    coi = article.find_all('em', string=lambda x: x and x.lower()=='conflict of interest')
    if len(coi) == 0:
        return None
    coi = article.find_all('em', string=lambda x: x and x.lower()=='conflict of interest')[0].find_parent()
    coiText = coi.get_text()
    next = coi.find_next_sibling()
    while next in article.find_all('p', {'class': 'chapter-para'}):
        coiText += ' ' + next.get_text()
        next = next.find_next_sibling()

    funding = article.find_all(class_ = 'section-title js-splitscreen-section-title')
    for x in funding:
        if 'Funding' in x:
            coiText += ' Funding: ' + x.find_next_sibling().get_text()
    print(coiText)

articleList = soup.find_all('div', {'class': 'sr-list al-article-box al-normal clearfix'})
for child in articleList:
    link = child.find('a').get('href')
    link = 'https://academic.oup.com' + link
    print(link)
    getDisclosure(link)